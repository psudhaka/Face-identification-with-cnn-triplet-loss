{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T14:02:42.625357Z",
     "start_time": "2018-03-26T22:02:42.622348+08:00"
    }
   },
   "source": [
    "### First we train a cnn to extract the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T01:53:17.756351Z",
     "start_time": "2018-03-28T09:51:03.650800+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zjuyang\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train: 444\n",
      "n_test: 46\n",
      "n_subjects: 24\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "batch_normalization_1 (Batch (None, 227, 227, 3)       12        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 227, 227, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 227, 227, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 113, 113, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 111, 111, 64)      18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 111, 111, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 55, 55, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 55, 55, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 55, 55, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 27, 27, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 25, 25, 32)        36896     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 25, 25, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 24)                1560      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 24)                0         \n",
      "=================================================================\n",
      "Total params: 133,828\n",
      "Trainable params: 133,822\n",
      "Non-trainable params: 6\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zjuyang\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:48: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "c:\\users\\zjuyang\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:48: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., steps_per_epoch=27, epochs=30, callbacks=[<keras.ca..., validation_data=[array([[[...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "27/27 [==============================] - 5s 173ms/step - loss: 0.3445 - acc: 0.8739 - val_loss: 0.1921 - val_acc: 0.9583\n",
      "Epoch 2/30\n",
      "27/27 [==============================] - 4s 149ms/step - loss: 0.1873 - acc: 0.9583 - val_loss: 0.1822 - val_acc: 0.9583\n",
      "Epoch 3/30\n",
      "27/27 [==============================] - 4s 159ms/step - loss: 0.1798 - acc: 0.9583 - val_loss: 0.1797 - val_acc: 0.9583\n",
      "Epoch 4/30\n",
      "27/27 [==============================] - 4s 155ms/step - loss: 0.1774 - acc: 0.9583 - val_loss: 0.1915 - val_acc: 0.9583\n",
      "Epoch 5/30\n",
      "27/27 [==============================] - 4s 153ms/step - loss: 0.1791 - acc: 0.9583 - val_loss: 0.1806 - val_acc: 0.9583\n",
      "Epoch 6/30\n",
      "27/27 [==============================] - 4s 153ms/step - loss: 0.1718 - acc: 0.9583 - val_loss: 0.1722 - val_acc: 0.9583\n",
      "Epoch 7/30\n",
      "27/27 [==============================] - 4s 152ms/step - loss: 0.1692 - acc: 0.9583 - val_loss: 0.1713 - val_acc: 0.9583\n",
      "Epoch 8/30\n",
      "27/27 [==============================] - 4s 150ms/step - loss: 0.1651 - acc: 0.9583 - val_loss: 0.1713 - val_acc: 0.9583\n",
      "Epoch 9/30\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 0.1631 - acc: 0.9583 - val_loss: 0.1686 - val_acc: 0.9583\n",
      "Epoch 10/30\n",
      "27/27 [==============================] - 4s 150ms/step - loss: 0.1616 - acc: 0.9585 - val_loss: 0.1623 - val_acc: 0.9583\n",
      "Epoch 11/30\n",
      "27/27 [==============================] - 4s 151ms/step - loss: 0.1574 - acc: 0.9581 - val_loss: 0.1630 - val_acc: 0.9583\n",
      "Epoch 12/30\n",
      "27/27 [==============================] - 4s 153ms/step - loss: 0.1520 - acc: 0.9584 - val_loss: 0.1573 - val_acc: 0.9592\n",
      "Epoch 13/30\n",
      "27/27 [==============================] - 4s 159ms/step - loss: 0.1499 - acc: 0.9587 - val_loss: 0.1541 - val_acc: 0.9592\n",
      "Epoch 14/30\n",
      "27/27 [==============================] - 4s 151ms/step - loss: 0.1470 - acc: 0.9589 - val_loss: 0.1552 - val_acc: 0.9574\n",
      "Epoch 15/30\n",
      "27/27 [==============================] - 4s 151ms/step - loss: 0.1443 - acc: 0.9596 - val_loss: 0.1470 - val_acc: 0.9601\n",
      "Epoch 16/30\n",
      "27/27 [==============================] - 4s 148ms/step - loss: 0.1407 - acc: 0.9588 - val_loss: 0.1425 - val_acc: 0.9592\n",
      "Epoch 17/30\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 0.1337 - acc: 0.9599 - val_loss: 0.1451 - val_acc: 0.9601\n",
      "Epoch 18/30\n",
      "27/27 [==============================] - 4s 150ms/step - loss: 0.1301 - acc: 0.9605 - val_loss: 0.1386 - val_acc: 0.9601\n",
      "Epoch 19/30\n",
      "27/27 [==============================] - 4s 151ms/step - loss: 0.1303 - acc: 0.9612 - val_loss: 0.1374 - val_acc: 0.9620\n",
      "Epoch 20/30\n",
      "27/27 [==============================] - 4s 148ms/step - loss: 0.1232 - acc: 0.9615 - val_loss: 0.1314 - val_acc: 0.9629\n",
      "Epoch 21/30\n",
      "27/27 [==============================] - 4s 155ms/step - loss: 0.1231 - acc: 0.9618 - val_loss: 0.1397 - val_acc: 0.9592\n",
      "Epoch 22/30\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 0.1228 - acc: 0.9620 - val_loss: 0.1409 - val_acc: 0.9638\n",
      "Epoch 23/30\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 0.1192 - acc: 0.9623 - val_loss: 0.1218 - val_acc: 0.9611\n",
      "Epoch 24/30\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 0.1065 - acc: 0.9652 - val_loss: 0.1153 - val_acc: 0.9674\n",
      "Epoch 25/30\n",
      "27/27 [==============================] - 4s 163ms/step - loss: 0.1182 - acc: 0.9635 - val_loss: 0.1185 - val_acc: 0.9638\n",
      "Epoch 26/30\n",
      "27/27 [==============================] - 4s 153ms/step - loss: 0.1105 - acc: 0.9653 - val_loss: 0.1221 - val_acc: 0.9638\n",
      "Epoch 27/30\n",
      "27/27 [==============================] - 4s 157ms/step - loss: 0.1095 - acc: 0.9646 - val_loss: 0.1104 - val_acc: 0.9701\n",
      "Epoch 28/30\n",
      "27/27 [==============================] - 4s 152ms/step - loss: 0.1065 - acc: 0.9656 - val_loss: 0.1073 - val_acc: 0.9665\n",
      "Epoch 29/30\n",
      "27/27 [==============================] - 4s 155ms/step - loss: 0.1003 - acc: 0.9670 - val_loss: 0.1053 - val_acc: 0.9656\n",
      "Epoch 30/30\n",
      "27/27 [==============================] - 4s 154ms/step - loss: 0.1000 - acc: 0.9677 - val_loss: 0.1002 - val_acc: 0.9665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24161febe10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os.path\n",
    "import numpy as np\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from model import build_my_cnn\n",
    "\n",
    "WEIGHTS_DIR = 'data/weights/'\n",
    "\n",
    "NB_EPOCH = 30\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "AUGMENTATION = True\n",
    "\n",
    "train_x, train_y = np.load('data/train_x.npy'), np.load('data/train_y.npy')\n",
    "test_x, test_y = np.load('data/test_x.npy'), np.load('data/test_y.npy')\n",
    "\n",
    "n_subjects = len(set(train_y))\n",
    "n_train = train_x.shape[0]\n",
    "n_test = test_x.shape[0]\n",
    "\n",
    "One = OneHotEncoder()\n",
    "One.fit(train_y.reshape(-1, 1))\n",
    "\n",
    "train_y = One.transform(train_y.reshape(-1, 1)).todense()\n",
    "test_y = One.transform(test_y.reshape(-1, 1)).todense()\n",
    "\n",
    "print('n_train: {}'.format(n_train))\n",
    "print('n_test: {}'.format(n_test))\n",
    "print('n_subjects: {}'.format(n_subjects))\n",
    "\n",
    "checkpoint = ModelCheckpoint(WEIGHTS_DIR + 'weights.best.h5', monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "model = build_my_cnn(227, n_subjects)\n",
    "model.summary()\n",
    "\n",
    "model.fit_generator(datagen.flow(train_x, train_y, batch_size=BATCH_SIZE),\n",
    "                        samples_per_epoch=train_x.shape[0],\n",
    "                        nb_epoch=NB_EPOCH,\n",
    "                        validation_data=[test_x, test_y],\n",
    "                        callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we train a more accuracy model with triplet loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T01:54:25.539292Z",
     "start_time": "2018-03-28T09:53:43.354651+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\DL\\DeepLearning\\faceID\\face-tpe\\model.py:67: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, use_bias=False, weights=[array([[-..., activation=\"linear\", input_dim=24)`\n",
      "  base_model.add(Dense(n_out, input_dim=n_in, bias=False, weights=[W_pca], activation='linear'))\n",
      "C:\\DL\\DeepLearning\\faceID\\face-tpe\\model.py:74: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  e = merge([a_emb, p_emb, n_emb], mode=triplet_merge, output_shape=triplet_merge_shape)\n",
      "c:\\users\\zjuyang\\anaconda3\\lib\\site-packages\\keras\\legacy\\layers.py:464: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "C:\\DL\\DeepLearning\\faceID\\face-tpe\\model.py:76: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"me..., inputs=[<tf.Tenso...)`\n",
      "  model = Model(input=[a, p, n], output=e)\n",
      "C:\\DL\\DeepLearning\\faceID\\face-tpe\\model.py:77: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"se..., inputs=Tensor(\"in...)`\n",
      "  predict = Model(input=a, output=a_emb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 1s 998us/step - loss: 0.5587\n",
      "EER: 13.63\n",
      "epoch: 1\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 727us/step - loss: 0.4711\n",
      "EER: 16.87\n",
      "epoch: 2\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 746us/step - loss: 0.4729\n",
      "EER: 14.60\n",
      "epoch: 3\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 751us/step - loss: 0.4633\n",
      "EER: 15.36\n",
      "epoch: 4\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 785us/step - loss: 0.4772\n",
      "EER: 17.20\n",
      "epoch: 5\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 769us/step - loss: 0.4596\n",
      "EER: 17.31\n",
      "epoch: 6\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 789us/step - loss: 0.4689\n",
      "EER: 17.63\n",
      "epoch: 7\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 778us/step - loss: 0.4704\n",
      "EER: 18.71\n",
      "epoch: 8\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 771us/step - loss: 0.4599\n",
      "EER: 17.96\n",
      "epoch: 9\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 724us/step - loss: 0.4668\n",
      "EER: 18.06\n",
      "epoch: 10\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 805us/step - loss: 0.4532\n",
      "EER: 16.98\n",
      "epoch: 11\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 765us/step - loss: 0.4696\n",
      "EER: 17.31\n",
      "epoch: 12\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 753us/step - loss: 0.4730\n",
      "EER: 20.01\n",
      "epoch: 13\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 769us/step - loss: 0.4457\n",
      "EER: 20.44\n",
      "epoch: 14\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 864us/step - loss: 0.4588\n",
      "EER: 19.04\n",
      "epoch: 15\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 774us/step - loss: 0.4463\n",
      "EER: 20.12\n",
      "epoch: 16\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 769us/step - loss: 0.4684\n",
      "EER: 24.01\n",
      "epoch: 17\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 766us/step - loss: 0.4599\n",
      "EER: 19.58\n",
      "epoch: 18\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 771us/step - loss: 0.4469\n",
      "EER: 23.58\n",
      "epoch: 19\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 816us/step - loss: 0.4553\n",
      "EER: 20.77\n",
      "epoch: 20\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 775us/step - loss: 0.4568\n",
      "EER: 22.17\n",
      "epoch: 21\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 838us/step - loss: 0.4630\n",
      "EER: 21.96\n",
      "epoch: 22\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 801us/step - loss: 0.4532\n",
      "EER: 19.90\n",
      "epoch: 23\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 803us/step - loss: 0.4649\n",
      "EER: 23.15\n",
      "epoch: 24\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 784us/step - loss: 0.4577\n",
      "EER: 20.44\n",
      "epoch: 25\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 795us/step - loss: 0.4508\n",
      "EER: 23.04\n",
      "epoch: 26\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 777us/step - loss: 0.4538\n",
      "EER: 21.85\n",
      "epoch: 27\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 784us/step - loss: 0.4292\n",
      "EER: 22.61\n",
      "epoch: 28\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 723us/step - loss: 0.4394\n",
      "EER: 24.45\n",
      "epoch: 29\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 740us/step - loss: 0.4611\n",
      "EER: 23.04\n",
      "epoch: 30\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 780us/step - loss: 0.4457\n",
      "EER: 24.55\n",
      "epoch: 31\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 756us/step - loss: 0.4353\n",
      "EER: 24.66\n",
      "epoch: 32\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 746us/step - loss: 0.4554\n",
      "EER: 25.31\n",
      "epoch: 33\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 729us/step - loss: 0.4641\n",
      "EER: 25.74\n",
      "epoch: 34\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 725us/step - loss: 0.4625\n",
      "EER: 26.18\n",
      "epoch: 35\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 795us/step - loss: 0.4406\n",
      "EER: 25.10\n",
      "epoch: 36\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 778us/step - loss: 0.4546\n",
      "EER: 27.15\n",
      "epoch: 37\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 751us/step - loss: 0.4389\n",
      "EER: 26.18\n",
      "epoch: 38\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 735us/step - loss: 0.4592\n",
      "EER: 25.74\n",
      "epoch: 39\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 773us/step - loss: 0.4372\n",
      "EER: 24.88\n",
      "epoch: 40\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 781us/step - loss: 0.4422\n",
      "EER: 24.99\n",
      "epoch: 41\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 730us/step - loss: 0.4532\n",
      "EER: 26.28\n",
      "epoch: 42\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 781us/step - loss: 0.4514\n",
      "EER: 26.50\n",
      "epoch: 43\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 764us/step - loss: 0.4414\n",
      "EER: 28.34\n",
      "epoch: 44\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 703us/step - loss: 0.4284\n",
      "EER: 27.37\n",
      "epoch: 45\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 766us/step - loss: 0.4422\n",
      "EER: 28.77\n",
      "epoch: 46\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 709us/step - loss: 0.4583\n",
      "EER: 26.28\n",
      "epoch: 47\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 725us/step - loss: 0.4446\n",
      "EER: 28.45\n",
      "epoch: 48\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 774us/step - loss: 0.4475\n",
      "EER: 28.34\n",
      "epoch: 49\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 772us/step - loss: 0.4519\n",
      "EER: 27.58\n",
      "epoch: 50\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 817us/step - loss: 0.4608\n",
      "EER: 27.37\n",
      "epoch: 51\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 740us/step - loss: 0.4547\n",
      "EER: 28.56\n",
      "epoch: 52\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 764us/step - loss: 0.4640\n",
      "EER: 27.47\n",
      "epoch: 53\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 750us/step - loss: 0.4508\n",
      "EER: 27.47\n",
      "epoch: 54\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 727us/step - loss: 0.4368\n",
      "EER: 27.47\n",
      "epoch: 55\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 759us/step - loss: 0.4330\n",
      "EER: 27.04\n",
      "epoch: 56\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 750us/step - loss: 0.4469\n",
      "EER: 28.02\n",
      "epoch: 57\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 772us/step - loss: 0.4375\n",
      "EER: 29.85\n",
      "epoch: 58\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 727us/step - loss: 0.4554\n",
      "EER: 29.31\n",
      "epoch: 59\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 758us/step - loss: 0.4448\n",
      "EER: 26.39\n",
      "epoch: 60\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 744us/step - loss: 0.4479\n",
      "EER: 26.83\n",
      "epoch: 61\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 736us/step - loss: 0.4472\n",
      "EER: 29.85\n",
      "epoch: 62\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 786us/step - loss: 0.4511\n",
      "EER: 28.56\n",
      "epoch: 63\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 707us/step - loss: 0.4476\n",
      "EER: 27.15\n",
      "epoch: 64\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 836us/step - loss: 0.4657\n",
      "EER: 29.64\n",
      "epoch: 65\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 750us/step - loss: 0.4447\n",
      "EER: 29.75\n",
      "epoch: 66\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 770us/step - loss: 0.4194\n",
      "EER: 28.77\n",
      "epoch: 67\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 715us/step - loss: 0.4425\n",
      "EER: 28.12\n",
      "epoch: 68\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 772us/step - loss: 0.4277\n",
      "EER: 29.10\n",
      "epoch: 69\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 762us/step - loss: 0.4564\n",
      "EER: 24.45\n",
      "epoch: 70\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 723us/step - loss: 0.4493\n",
      "EER: 25.96\n",
      "epoch: 71\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 719us/step - loss: 0.4294\n",
      "EER: 27.91\n",
      "epoch: 72\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 790us/step - loss: 0.4466\n",
      "EER: 26.61\n",
      "epoch: 73\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 768us/step - loss: 0.4499\n",
      "EER: 27.15\n",
      "epoch: 74\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 754us/step - loss: 0.4459\n",
      "EER: 26.39\n",
      "epoch: 75\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 727us/step - loss: 0.4279\n",
      "EER: 30.61\n",
      "epoch: 76\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 735us/step - loss: 0.4491\n",
      "EER: 32.13\n",
      "epoch: 77\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 774us/step - loss: 0.4524\n",
      "EER: 29.85\n",
      "epoch: 78\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 774us/step - loss: 0.4538\n",
      "EER: 28.01\n",
      "epoch: 79\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512/512 [==============================] - 0s 690us/step - loss: 0.4309\n",
      "EER: 25.64\n",
      "epoch: 80\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 740us/step - loss: 0.4549\n",
      "EER: 31.15\n",
      "epoch: 81\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 782us/step - loss: 0.4351\n",
      "EER: 23.04\n",
      "epoch: 82\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 766us/step - loss: 0.4351\n",
      "EER: 32.23\n",
      "epoch: 83\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 751us/step - loss: 0.4388\n",
      "EER: 28.45\n",
      "epoch: 84\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 770us/step - loss: 0.4356\n",
      "EER: 29.10\n",
      "epoch: 85\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 737us/step - loss: 0.4341\n",
      "EER: 28.77\n",
      "epoch: 86\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 780us/step - loss: 0.4402\n",
      "EER: 30.61\n",
      "epoch: 87\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 741us/step - loss: 0.4415\n",
      "EER: 28.01\n",
      "epoch: 88\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 749us/step - loss: 0.4494\n",
      "EER: 29.75\n",
      "epoch: 89\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 778us/step - loss: 0.4320\n",
      "EER: 30.39\n",
      "epoch: 90\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 770us/step - loss: 0.4508\n",
      "EER: 30.07\n",
      "epoch: 91\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 737us/step - loss: 0.4268\n",
      "EER: 28.88\n",
      "epoch: 92\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 723us/step - loss: 0.4268\n",
      "EER: 31.48\n",
      "epoch: 93\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 705us/step - loss: 0.4428\n",
      "EER: 28.88\n",
      "epoch: 94\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 760us/step - loss: 0.4442\n",
      "EER: 30.40\n",
      "epoch: 95\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 799us/step - loss: 0.4553\n",
      "EER: 29.31\n",
      "epoch: 96\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 767us/step - loss: 0.4332\n",
      "EER: 27.69\n",
      "epoch: 97\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 753us/step - loss: 0.4375\n",
      "EER: 31.58\n",
      "epoch: 98\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 761us/step - loss: 0.4386\n",
      "EER: 28.77\n",
      "epoch: 99\n",
      "Epoch 1/1\n",
      "512/512 [==============================] - 0s 758us/step - loss: 0.4461\n",
      "EER: 31.26\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "from model import build_my_cnn\n",
    "from model import build_tpe\n",
    "from model import Bottleneck\n",
    "from identification import get_scores, calc_metrics\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_in = 24\n",
    "n_out = 24\n",
    "\n",
    "data_dir='data/'\n",
    "\n",
    "cnn = build_my_cnn(227, 24)\n",
    "cnn.load_weights(data_dir+'weights/weights.best.h5')\n",
    "bottleneck = Bottleneck(cnn, ~1)\n",
    "\n",
    "train_x, train_y = np.load(data_dir+'train_x.npy'), np.load(data_dir+'train_y.npy')\n",
    "test_x, test_y = np.load(data_dir+'test_x.npy'), np.load(data_dir+'test_y.npy')\n",
    "\n",
    "train_x = np.vstack([train_x, test_x])\n",
    "train_y = np.hstack([train_y, test_y])\n",
    "\n",
    "dev_x = np.load(data_dir+'dev_x.npy')\n",
    "dev_protocol = np.load(data_dir+'dev_protocol.npy')\n",
    "\n",
    "train_emb = bottleneck.predict(train_x, batch_size=256)\n",
    "dev_emb = bottleneck.predict(dev_x, batch_size=256)\n",
    "\n",
    "del train_x\n",
    "\n",
    "pca = PCA(n_out)\n",
    "pca.fit(train_emb)\n",
    "W_pca = pca.components_\n",
    "\n",
    "tpe, tpe_pred = build_tpe(n_in, n_out, W_pca.T)\n",
    "# tpe.load_weights('data/weights/weights.tpe.mineer.h5')\n",
    "\n",
    "train_y = np.array(train_y)\n",
    "subjects = list(set(train_y))\n",
    "\n",
    "anchors_inds = []\n",
    "positives_inds = []\n",
    "labels = []\n",
    "\n",
    "for subj in subjects:\n",
    "    mask = train_y == subj\n",
    "    inds = np.where(mask)[0]\n",
    "    for a, p in itertools.permutations(inds, 2):\n",
    "        anchors_inds.append(a)\n",
    "        positives_inds.append(p)\n",
    "        labels.append(subj)\n",
    "\n",
    "anchors = train_emb[anchors_inds]\n",
    "positives = train_emb[positives_inds]\n",
    "n_anchors = len(anchors_inds)\n",
    "\n",
    "NB_EPOCH = 100\n",
    "COLD_START = NB_EPOCH\n",
    "BATCH_SIZE = 4\n",
    "BIG_BATCH_SIZE = 512\n",
    "\n",
    "inds = np.arange(n_anchors)\n",
    "\n",
    "def get_batch(hard=False):\n",
    "    batch_inds = np.random.choice(inds, size=BIG_BATCH_SIZE, replace=False)\n",
    "\n",
    "    train_emb2 = tpe_pred.predict(train_emb, batch_size=1024)\n",
    "    scores = train_emb2 @ train_emb2.T\n",
    "    negative_inds = []\n",
    "\n",
    "    for i in batch_inds:\n",
    "        label = labels[i]\n",
    "        mask = train_y == label\n",
    "        if hard:\n",
    "            negative_inds.append(np.ma.array(scores[label], mask=mask).argmax())\n",
    "        else:\n",
    "            negative_inds.append(np.random.choice(np.where(np.logical_not(mask))[0], size=1)[0])\n",
    "\n",
    "    return anchors[batch_inds], positives[batch_inds], train_emb[negative_inds]\n",
    "\n",
    "\n",
    "def test():\n",
    "    dev_emb2 = tpe_pred.predict(dev_emb)\n",
    "    tsc, isc = get_scores(dev_emb2, dev_protocol)\n",
    "    eer, _, _, _ = calc_metrics(tsc, isc)\n",
    "    return eer\n",
    "\n",
    "z = np.zeros((BIG_BATCH_SIZE,))\n",
    "\n",
    "mineer = float('inf')\n",
    "\n",
    "for e in range(NB_EPOCH):\n",
    "    print('epoch: {}'.format(e))\n",
    "    a, p, n = get_batch(e > COLD_START)\n",
    "    tpe.fit([a, p, n], z, batch_size=BATCH_SIZE, epochs=1)\n",
    "    eer = test()\n",
    "    print('EER: {:.2f}'.format(eer * 100))\n",
    "    if eer < mineer:\n",
    "        mineer = eer\n",
    "        tpe.save_weights(data_dir+'weights/weights.tpe.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-26T14:14:24.873952Z",
     "start_time": "2018-03-26T22:14:24.870975+08:00"
    }
   },
   "source": [
    "### Let's test some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-28T02:01:21.065733Z",
     "start_time": "2018-03-28T10:01:15.769519+08:00"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\DL\\DeepLearning\\faceID\\face-tpe\\model.py:67: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(24, use_bias=False, weights=[array([[0..., activation=\"linear\", input_dim=24)`\n",
      "  base_model.add(Dense(n_out, input_dim=n_in, bias=False, weights=[W_pca], activation='linear'))\n",
      "C:\\DL\\DeepLearning\\faceID\\face-tpe\\model.py:74: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  e = merge([a_emb, p_emb, n_emb], mode=triplet_merge, output_shape=triplet_merge_shape)\n",
      "c:\\users\\zjuyang\\anaconda3\\lib\\site-packages\\keras\\legacy\\layers.py:464: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "C:\\DL\\DeepLearning\\faceID\\face-tpe\\model.py:76: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"me..., inputs=[<tf.Tenso...)`\n",
      "  model = Model(input=[a, p, n], output=e)\n",
      "C:\\DL\\DeepLearning\\faceID\\face-tpe\\model.py:77: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"se..., inputs=Tensor(\"in...)`\n",
      "  predict = Model(input=a, output=a_emb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rects on image 0: [rectangle(171,234,449,511)]\n",
      "Rects on image 1: [rectangle(425,272,656,503)]\n",
      "Embeddings of faces on image 0:\n",
      "[array([ 0.13418996, -0.6970258 ,  0.3399457 , -0.10224586,  0.19762553,\n",
      "       -0.4532321 , -0.11152461,  0.10998376,  0.14906865, -0.13990404,\n",
      "       -0.06401891,  0.02188361, -0.13371314,  0.04735205, -0.04986608,\n",
      "       -0.01053285,  0.00349552, -0.04953917, -0.08118474, -0.09452056,\n",
      "        0.07530235,  0.00160445,  0.08239461, -0.03971564], dtype=float32)]\n",
      "Embeddings of faces on image 1:\n",
      "[array([ 0.24917145, -0.423666  ,  0.45562866, -0.22780605,  0.31777102,\n",
      "       -0.52102154,  0.06990661,  0.06385479,  0.07251784, -0.24117978,\n",
      "       -0.03866799, -0.00900814, -0.11571075,  0.02112562, -0.02454004,\n",
      "       -0.00770312,  0.02071871, -0.06449758, -0.0772385 , -0.08955176,\n",
      "        0.0880991 , -0.03264282,  0.07646518, -0.06908865], dtype=float32)]\n",
      "Score matrix:\n",
      "[[0.90333]]\n",
      "Decision matrix :\n",
      "[[ True]]\n"
     ]
    }
   ],
   "source": [
    "from model import FaceVerificator\n",
    "from skimage import io\n",
    "\n",
    "###\n",
    "img_path_0 = 'data/dev/21.jpg'\n",
    "img_path_1 = 'data/dev/22.jpg'\n",
    "dist = 0.85\n",
    "###\n",
    "\n",
    "extractor = FaceVerificator('model')\n",
    "extractor.initialize_model()\n",
    "\n",
    "img_0 = io.imread(img_path_0)\n",
    "img_1 = io.imread(img_path_1)\n",
    "\n",
    "faces_0 = extractor.process_image(img_0)\n",
    "faces_1 = extractor.process_image(img_1)\n",
    "\n",
    "n_faces_0 = len(faces_0)\n",
    "n_faces_1 = len(faces_1)\n",
    "\n",
    "if n_faces_0 == 0 or n_faces_1 == 0:\n",
    "    print('Error: No faces found on the {}!'.format(img_path_0 if n_faces_0 == 0 else img_path_1))\n",
    "    exit()\n",
    "\n",
    "rects_0 = list(map(lambda p: p[0], faces_0))\n",
    "rects_1 = list(map(lambda p: p[0], faces_1))\n",
    "\n",
    "embs_0 = list(map(lambda p: p[1], faces_0))\n",
    "embs_1 = list(map(lambda p: p[1], faces_1))\n",
    "\n",
    "scores, comps = extractor.compare_many(dist, embs_0, embs_1)\n",
    "\n",
    "print('Rects on image 0: {}'.format(rects_0))\n",
    "print('Rects on image 1: {}'.format(rects_1))\n",
    "\n",
    "print('Embeddings of faces on image 0:')\n",
    "print(embs_0)\n",
    "\n",
    "print('Embeddings of faces on image 1:')\n",
    "print(embs_1)\n",
    "\n",
    "print('Score matrix:')\n",
    "print(scores)\n",
    "\n",
    "print('Decision matrix :')\n",
    "print(comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
